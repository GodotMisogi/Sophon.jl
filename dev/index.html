<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · Sophon.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://MilkshakeForReal.github.io/Sophon.jl/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>Sophon.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="tutorials/discontinuous/">Fitting a nonlinear discontinuous function</a></li><li><a class="tocitem" href="tutorials/poisson/">1D Poisson&#39;s Equation</a></li><li><a class="tocitem" href="tutorials/convection/">1D Convection Equation</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/main/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Sophon"><a class="docs-heading-anchor" href="#Sophon">Sophon</a><a id="Sophon-1"></a><a class="docs-heading-anchor-permalink" href="#Sophon" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/MilkshakeForReal/Sophon.jl">Sophon</a>.</p><ul><li><a href="#Sophon.FourierFeature"><code>Sophon.FourierFeature</code></a></li><li><a href="#Sophon.PINNAttention"><code>Sophon.PINNAttention</code></a></li><li><a href="#Sophon.RBF"><code>Sophon.RBF</code></a></li><li><a href="#Sophon.Sine"><code>Sophon.Sine</code></a></li><li><a href="#Sophon.TriplewiseFusion"><code>Sophon.TriplewiseFusion</code></a></li><li><a href="#Sophon.FourierAttention"><code>Sophon.FourierAttention</code></a></li><li><a href="#Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T&lt;:Int64}"><code>Sophon.FullyConnected</code></a></li><li><a href="#Sophon.MultiscaleFourier-Union{Tuple{Int64}, Tuple{S}, Tuple{N2}, Tuple{N1}, Tuple{Int64, Tuple{Vararg{Int64, N1}}}, Tuple{Int64, Tuple{Vararg{Int64, N1}}, Function}} where {N1, N2, S}"><code>Sophon.MultiscaleFourier</code></a></li><li><a href="#Sophon.Siren"><code>Sophon.Siren</code></a></li><li><a href="#Sophon.SirenAttention"><code>Sophon.SirenAttention</code></a></li><li><a href="#Sophon.gaussian"><code>Sophon.gaussian</code></a></li><li><a href="#Sophon.kaiming_normal-Tuple{Random.AbstractRNG, Vararg{Integer}}"><code>Sophon.kaiming_normal</code></a></li><li><a href="#Sophon.kaiming_uniform-Tuple{Random.AbstractRNG, Vararg{Integer}}"><code>Sophon.kaiming_uniform</code></a></li><li><a href="#Sophon.wu"><code>Sophon.wu</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="Sophon.FourierFeature" href="#Sophon.FourierFeature"><code>Sophon.FourierFeature</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FourierFeature(in_dims::Int, modes::NTuple{N,Pair{S,T}}) where {N,S,T&lt;:Int}</code></pre><p>Fourier Feature Network.</p><p class="math-container">\[\phi^{(i)}(x)=\left[\sin \left(2 \pi W^{(i)} x\right) ; \cos 2 \pi W^{(i)} x\right], \quad W^{(i)} \sim \mathcal{N}\left(0, \sigma^{(i)}\right)\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Number of the input dimensions.</li><li><code>modes</code>: A tuple of pairs of <code>std =&gt; out_dims</code>, where <code>std</code> is the standard deviation of the Gaussian distribution, and <code>out_dims</code> is the corresponding number of output dimensions.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x</code>: An an AbstractArray with <code>size(x, 1) == in_dims</code>.</li></ul><p><strong>Returns</strong></p><ul><li>An <code>AbstractArray</code> with <code>size(y, 1) == sum(last(modes) * 2)</code>.</li><li>The states of the modes.</li></ul><p><strong>States</strong></p><ul><li>States of each mode wrapped in a NamedTuple with <code>fields = mode_1, mode_2, ..., mode_N</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; f = FourierFeature(2, (1 =&gt; 3, 50 =&gt; 4))
FourierFeature(2 =&gt; 14)

julia&gt; ps, st = Lux.setup(rng, f)
(NamedTuple(), (mode_1 = Float32[0.7510394 0.0678698; -1.6466209 -0.08511321; -0.4704813 2.0663197], mode_2 = Float32[-98.90031 -42.593884; 110.35572 15.565719; 81.60114 51.257904; -0.53021294 15.216658]))

julia&gt; st
(mode_1 = Float32[0.7510394 0.0678698; -1.6466209 -0.08511321; -0.4704813 2.0663197], mode_2 = Float32[-98.90031 -42.593884; 110.35572 15.565719; 81.60114 51.257904; -0.53021294 15.216658])</code></pre><p><strong>References</strong></p><p>[1] Tancik, Matthew, et al. “Fourier features let networks learn high frequency functions in low dimensional domains.” Advances in Neural Information Processing Systems 33 (2020): 7537-7547.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/basic.jl#L1-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.PINNAttention" href="#Sophon.PINNAttention"><code>Sophon.PINNAttention</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PINNAttention(H_net, U_net, V_net, fusion_layers)
PINNAttention(in_dims::Int, out_dims::Int, activation::Function=sin;
              hidden_dims::Int, num_layers::Int)</code></pre><p>The output dimesion of <code>H_net</code> and the input dimension of <code>fusion_layers</code> must be the same. For the second and the third constructor, <code>Dense</code> layers is used for <code>H_net</code>, <code>U_net</code>, and <code>V_net</code>. Note that the first constructer does not contain the output layer.</p><pre><code class="nohighlight hljs">                 x → U_net → u                           u
                               ↘                           ↘
x → H_net →  h1 → fusionlayer1 → connection → fusionlayer2 → connection
                               ↗                           ↗
                 x → V_net → v                           v</code></pre><p><strong>Arguments</strong></p><pre><code class="nohighlight hljs">- `H_net`: `AbstractExplicitLayer`.
- `U_net`: `AbstractExplicitLayer`.
- `V_net`: `AbstractExplicitLayer`.
- `fusion_layers`: `Chain`.</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><code>num_layers</code>: The number of hidden layers.</li><li><code>hidden_dims</code>: The number of hidden dimensions of each hidden layer.</li></ul><p><strong>References</strong></p><p>[1] Wang, Sifan, Yujun Teng, and Paris Perdikaris. &quot;Understanding and mitigating gradient flow pathologies in physics-informed neural networks.&quot; SIAM Journal on Scientific Computing 43.5 (2021): A3055-A3081</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/nets.jl#L1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.RBF" href="#Sophon.RBF"><code>Sophon.RBF</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RBF(in_dims::Int, out_dims::Int, num_centers::Int=out_dims; sigma::AbstractFloat=0.2f0)</code></pre><p>Radial Basis Fuction Network.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/basic.jl#L268-L272">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.Sine" href="#Sophon.Sine"><code>Sophon.Sine</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Sine(in_dims::Int, out_dims::Int, activation=sin; is_first::Bool = false, omega::AbstractFloat = 30f0)</code></pre><p>Sinusoidal layer.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">s = Sine(2, 2; is_first=true) # first layer
s = Sine(2, 2) # hidden layer
s = Sine(2, 2, identity) # last layer</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/basic.jl#L178-L190">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.TriplewiseFusion" href="#Sophon.TriplewiseFusion"><code>Sophon.TriplewiseFusion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TriplewiseFusion(connection, layers...)</code></pre><pre><code class="nohighlight hljs">         u1                    u2
            ↘                     ↘
h1 → layer1 → connection → layer2 → connection
            ↗                     ↗
         v1                    v2</code></pre><p><strong>Arguments</strong></p><ul><li><code>connection</code>: A functio takes 3 inputs and combines them.</li><li><code>layers</code>: <code>AbstractExplicitLayer</code>s or a <code>Chain</code>.</li></ul><p><strong>Inputs</strong></p><p>Layer behaves differently based on input type:</p><ol><li>A tripe of <code>(h, u, v)</code>, where <code>u</code> and <code>v</code> itself are tuples of length <code>N</code>, the <code>layers</code> is also a tuple of length <code>N</code>. The computation is as follows</li></ol><pre><code class="language-julia hljs">for i in 1:N
    h = connection(layers[i](h), u[i], v[i])
end</code></pre><ol><li>A triple of <code>(h, u, v)</code>, where <code>u</code> and <code>v</code> are <code>AbstractArray</code>s.</li></ol><pre><code class="language-julia hljs">for i in 1:N
    h = connection(layers[i](h), u, v)
end</code></pre><p><strong>Parameters</strong></p><ul><li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code></li></ul><p><strong>States</strong></p><ul><li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/basic.jl#L87-L133">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.FourierAttention" href="#Sophon.FourierAttention"><code>Sophon.FourierAttention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">FourierAttention(in_dims::Int, out_dims::Int, activation::Function=sin;
                 hidden_dims::Int=512, num_layers::Int=6, modes::NTuple)</code></pre><pre><code class="nohighlight hljs">x → [FourierFeature(x); x] → PINNAttention</code></pre><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: The input dimension.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>modes</code>: A tuple of pairs of random frequencies and the number of samples.</li><li><code>hidden_dim</code>: The hidden dimension of each hidden layer.</li><li><code>num_layers</code>: The number of hidden layers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; FourierAttention(3, 1, sin; hidden_dims=10, num_layers=3,
                        modes=(1 =&gt; 10, 10 =&gt; 10, 50 =&gt; 10))
Chain(
    layer_1 = SkipConnection(
        FourierFeature(3 =&gt; 60),
        vcat
    ),
    layer_2 = PINNAttention(
        H_net = Dense(63 =&gt; 10, sin),   # 640 parameters
        U_net = Dense(63 =&gt; 10, sin),   # 640 parameters
        V_net = Dense(63 =&gt; 10, sin),   # 640 parameters
        fusion = TriplewiseFusion(
            layers = (layer_1 = Dense(10 =&gt; 10, sin), layer_2 = Dense(10 =&gt; 10, sin), layer_3 = Dense(10 =&gt; 10, sin), layer_4 = Dense(10 =&gt; 1)),  # 341 parameters
        ),
    ),
)         # Total: 2_261 parameters,
          #        plus 90 states, summarysize 176 bytes.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/nets.jl#L74-L113">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T&lt;:Int64}" href="#Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T&lt;:Int64}"><code>Sophon.FullyConnected</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">FullyConnected(layer_dims::NTuple{N, Int}, activation; outermost = true, init_weight = kaiming_uniform)
FullyConnected(in_dims::Int, out_dims::Int, activation::Function;
               hidden_dims::Int, num_layers::Int, outermost=true, init_weight = kaiming_uniform)</code></pre><p>Create fully connected layers.</p><p><strong>Arguments</strong></p><ul><li><code>layer_dims</code>: Number of dimensions of each layer.</li><li><code>hidden_dims</code>: Number of hidden dimensions.</li><li><code>num_layers</code>: Number of layers.</li><li><code>activation</code>: Activation function.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>outermost</code>: Whether to use activation function for the last layer. If <code>false</code>, the activation function is applied to the output of the last layer.</li><li><code>init_weight</code>: Initialization method for the weights.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; fc = FullyConnected((1, 12, 24, 32), relu)
Chain(
    layer_1 = Dense(1 =&gt; 12, relu),     # 24 parameters
    layer_2 = Dense(12 =&gt; 24, relu),    # 312 parameters
    layer_3 = Dense(24 =&gt; 32),          # 800 parameters
)         # Total: 1_136 parameters,
          #        plus 0 states, summarysize 48 bytes.

julia&gt; fc = FullyConnected(1, 10, relu; hidden_dims=20, num_layers=3)
Chain(
    layer_1 = Dense(1 =&gt; 20, relu),     # 40 parameters
    layer_2 = Dense(20 =&gt; 20, relu),    # 420 parameters
    layer_3 = Dense(20 =&gt; 20, relu),    # 420 parameters
    layer_4 = Dense(20 =&gt; 10),          # 210 parameters
)         # Total: 1_090 parameters,
          #        plus 0 states, summarysize 64 bytes.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/nets.jl#L247-L287">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.MultiscaleFourier-Union{Tuple{Int64}, Tuple{S}, Tuple{N2}, Tuple{N1}, Tuple{Int64, Tuple{Vararg{Int64, N1}}}, Tuple{Int64, Tuple{Vararg{Int64, N1}}, Function}} where {N1, N2, S}" href="#Sophon.MultiscaleFourier-Union{Tuple{Int64}, Tuple{S}, Tuple{N2}, Tuple{N1}, Tuple{Int64, Tuple{Vararg{Int64, N1}}}, Tuple{Int64, Tuple{Vararg{Int64, N1}}, Function}} where {N1, N2, S}"><code>Sophon.MultiscaleFourier</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">MultiscaleFourier(in_dims::Int, layer_dims::NTuple, activation=identity, modes::NTuple)</code></pre><p>Multi-scale Fourier Feature Net.</p><pre><code class="nohighlight hljs">x → FourierFeature → FullyConnected → y</code></pre><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: The number of input dimensions.</li><li><code>layer_dims</code>: A tuple of hidden dimensions used to construct <code>FullyConnected</code>.</li><li><code>activation</code>: The activation function used to construct <code>FullyConnected</code>.</li><li><code>modes</code>: A tuple of modes used to construct <code>FourierFeature</code>.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>modes</code>: A tuple of modes used to construct <code>FourierFeature</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; m = MultiscaleFourier(2, (30, 30, 1), sin; modes=(1 =&gt; 10, 10 =&gt; 10, 50 =&gt; 10))
Chain(
    layer_1 = FourierFeature(2 =&gt; 60),
    layer_2 = Dense(60 =&gt; 30, sin),     # 1_830 parameters
    layer_3 = Dense(30 =&gt; 30, sin),     # 930 parameters
    layer_4 = Dense(30 =&gt; 1),           # 31 parameters
)         # Total: 2_791 parameters,
          #        plus 60 states, summarysize 112 bytes.</code></pre><p><strong>References</strong></p><p>[1] Wang, Sifan, Hanwen Wang, and Paris Perdikaris. “On the eigenvector bias of fourier feature networks: From regression to solving multi-scale pdes with physics-informed neural networks.” Computer Methods in Applied Mechanics and Engineering 384 (2021): 113938.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/nets.jl#L140-L176">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.Siren" href="#Sophon.Siren"><code>Sophon.Siren</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Siren(in_dims::Int, hidden_dim::Int, num_layers::Int; omega = 30f0)
Siren(layer_dims::NTuple{N, T}; omega = 30f0) where {N, T &lt;: Int}</code></pre><p>Sinusoidal Representation Network.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>omega</code>: The <code>ω₀</code> used for the first layer.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; Siren(3; hidden_dims=20, num_layers=3)
Chain(
    layer_1 = Sine(3 =&gt; 20),            # 80 parameters, plus 1
    layer_2 = Sine(20 =&gt; 20),           # 420 parameters
    layer_3 = Sine(20 =&gt; 20),           # 420 parameters
)         # Total: 920 parameters,
          #        plus 1 states, summarysize 292 bytes.

julia&gt; Siren(3, 1; hidden_dims=20, num_layers=3)
Chain(
    layer_1 = Sine(3 =&gt; 20),            # 80 parameters, plus 1
    layer_2 = Sine(20 =&gt; 20),           # 420 parameters
    layer_3 = Sine(20 =&gt; 20),           # 420 parameters
    layer_4 = Sine(20 =&gt; 1),            # 21 parameters
)         # Total: 941 parameters,
          #        plus 1 states, summarysize 388 bytes.</code></pre><p><strong>References</strong></p><p>[1] Sitzmann, Vincent, et al. &quot;Implicit neural representations with periodic activation functions.&quot; Advances in Neural Information Processing Systems 33 (2020): 7462-7473.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/nets.jl#L189-L223">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.SirenAttention" href="#Sophon.SirenAttention"><code>Sophon.SirenAttention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">SirenAttention(in_dims::Int, out_dims::Int, activation::Function=sin;
hidden_dims::Int=512, num_layers::Int=6)</code></pre><pre><code class="nohighlight hljs">x -&gt; Sine -&gt; PINNAttention</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/layers/nets.jl#L124-L131">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.gaussian" href="#Sophon.gaussian"><code>Sophon.gaussian</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gaussian(x, a=0.2)</code></pre><p>The Gaussian activation function.</p><p class="math-container">\[e^{\frac{-0.5 x^{2}}{a^{2}}}\]</p><p><strong>References</strong></p><p>[1] Ramasinghe, Sameera, and Simon Lucey. &quot;Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.&quot; arXiv preprint arXiv:2111.15135 (2021).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/activations.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.kaiming_normal-Tuple{Random.AbstractRNG, Vararg{Integer}}" href="#Sophon.kaiming_normal-Tuple{Random.AbstractRNG, Vararg{Integer}}"><code>Sophon.kaiming_normal</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kaiming_normal(rng::AbstractRNG, size...; gain = √2f0)</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers taken from a normal distribution standard deviation <code>gain / sqrt(fan_in)</code></p><p><strong>References</strong></p><p>[1] He, Kaiming, et al. &quot;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.&quot; <em>Proceedings of the IEEE international conference on computer vision</em>. 2015.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/utils.jl#L33-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.kaiming_uniform-Tuple{Random.AbstractRNG, Vararg{Integer}}" href="#Sophon.kaiming_uniform-Tuple{Random.AbstractRNG, Vararg{Integer}}"><code>Sophon.kaiming_uniform</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kaiming_uniform(rng::AbstractRNG, size...; gain = √2f0)</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a uniform distribution on the interval <code>[-x, x]</code>, where <code>x = gain * sqrt(3/fan_in)</code>.</p><p><strong>References</strong></p><p>[1] He, Kaiming, et al. &quot;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.&quot; <em>Proceedings of the IEEE international conference on computer vision</em>. 2015.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/utils.jl#L6-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.wu" href="#Sophon.wu"><code>Sophon.wu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">wu(x,a=1)</code></pre><p>An activation function I designed for use in coordinate-mlps.</p><p class="math-container">\[\frac{x\left(5 x^{2}-1\right)}{\left(1+x^{2}\right)^{4}}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/fdc22b66b58495f547e9b36db9e3e3b5cf330660/src/activations.jl#L19-L27">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="tutorials/discontinuous/">Fitting a nonlinear discontinuous function »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Tuesday 23 August 2022 06:18">Tuesday 23 August 2022</span>. Using Julia version 1.8.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
