<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Fitting a nonlinear discontinuous function · Sophon.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://YichengDWu.github.io/Sophon.jl/tutorials/discontinuous/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/indigo.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Sophon.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Fitting a nonlinear discontinuous function</a><ul class="internal"><li><a class="tocitem" href="#Import-pacakges"><span>Import pacakges</span></a></li><li><a class="tocitem" href="#Dataset"><span>Dataset</span></a></li><li><a class="tocitem" href="#Naive-Neural-Nets"><span>Naive Neural Nets</span></a></li><li><a class="tocitem" href="#Siren"><span>Siren</span></a></li><li><a class="tocitem" href="#Gaussian-activation-function"><span>Gaussian activation function</span></a></li><li><a class="tocitem" href="#Quadratic-activation-function"><span>Quadratic activation function</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li><li><a class="tocitem" href="../poisson/">1D Multi-scale Poisson&#39;s Equation</a></li><li><a class="tocitem" href="../convection/">1D Convection Equation</a></li><li><a class="tocitem" href="../helmholtz/">2D Helmholtz Equation</a></li><li><a class="tocitem" href="../allen_cahn/">Allen-Cahn Equation with Sequential Training</a></li><li><a class="tocitem" href="../SchrödingerEquation/">Schrödinger Equation: A PDE System with Resampling</a></li><li><a class="tocitem" href="../L_shape/">Poisson equation over an L-shaped domain</a></li><li><a class="tocitem" href="../waveinverse2/">Inverse problem for the wave equation with unknown velocity field</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Fitting a nonlinear discontinuous function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Fitting a nonlinear discontinuous function</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/YichengDWu/Sophon.jl/blob/main/docs/src/tutorials/discontinuous.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Fitting-a-nonlinear-discontinuous-function"><a class="docs-heading-anchor" href="#Fitting-a-nonlinear-discontinuous-function">Fitting a nonlinear discontinuous function</a><a id="Fitting-a-nonlinear-discontinuous-function-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-a-nonlinear-discontinuous-function" title="Permalink"></a></h1><p>This example is taken from <a href="https://royalsocietypublishing.org/doi/epdf/10.1098/rspa.2020.0334">here</a>. However, we do not use adaptive activation functions. Instead, we show that using suitable non-parametric activation functions immediately performs better.</p><p>Consider the following  discontinuous  function  with  discontinuity  at <span>$x=0$</span>:</p><p class="math-container">\[u(x)= \begin{cases}0.2 \sin (18 x) &amp; \text { if } x \leq 0 \\ 1+0.3 x \cos (54 x) &amp; \text { otherwise }\end{cases}\]</p><p>The domain is <span>$[-1,1]$</span>. The number of training points used is <code>50</code>.</p><h2 id="Import-pacakges"><a class="docs-heading-anchor" href="#Import-pacakges">Import pacakges</a><a id="Import-pacakges-1"></a><a class="docs-heading-anchor-permalink" href="#Import-pacakges" title="Permalink"></a></h2><pre><code class="language-julia hljs">using Lux, Sophon
using NNlib, Optimisers, Plots, Random, StatsBase, Zygote</code></pre><h2 id="Dataset"><a class="docs-heading-anchor" href="#Dataset">Dataset</a><a id="Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Dataset" title="Permalink"></a></h2><pre><code class="language-julia hljs">function u(x)
    if x &lt;= 0
        return 0.2 * sin(18 * x)
    else
        return 1 + 0.3 * x * cos(54 * x)
    end
end

function generate_data(n=50)
    x = reshape(collect(range(-1.0f0, 1.0f0, n)), (1, n))
    y = u.(x)
    return (x, y)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">generate_data (generic function with 2 methods)</code></pre><p>Let&#39;s visualize the data.</p><pre><code class="language-julia hljs">x_train, y_train = generate_data(50)
x_test, y_test = generate_data(200)
Plots.plot(vec(x_test), vec(y_test),label=false)</code></pre><p><img src="../u.svg" alt/></p><h2 id="Naive-Neural-Nets"><a class="docs-heading-anchor" href="#Naive-Neural-Nets">Naive Neural Nets</a><a id="Naive-Neural-Nets-1"></a><a class="docs-heading-anchor-permalink" href="#Naive-Neural-Nets" title="Permalink"></a></h2><p>First we demonstrate show naive fully connected neural nets could be really bad at fitting this function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), relu)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, relu),     # 100 parameters
    layer_2 = Dense(50 =&gt; 50, relu),    # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, relu),    # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, relu),    # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 80 bytes.</code></pre><h3 id="Train-the-model"><a class="docs-heading-anchor" href="#Train-the-model">Train the model</a><a id="Train-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-model" title="Permalink"></a></h3><pre><code class="language-julia hljs">function train(model, x, y)
    ps, st = Lux.setup(Random.default_rng(), model)
    opt = Adam()
    st_opt = Optimisers.setup(opt,ps)
    function loss(model, ps, st, x, y)
        y_pred, _ = model(x, ps, st)
        mes = mean(abs2, y_pred .- y)
        return mes
    end

    for i in 1:2000
        gs = gradient(p-&gt;loss(model,p,st,x,y), ps)[1]
        st_opt, ps = Optimisers.update(st_opt, ps, gs)
        if i % 100 == 1 || i == 2000
            println(&quot;Epoch $i ||  &quot;, loss(model,ps,st,x,y))
        end
    end
    return ps, st
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">train (generic function with 1 method)</code></pre><h3 id="Plot-the-result"><a class="docs-heading-anchor" href="#Plot-the-result">Plot the result</a><a id="Plot-the-result-1"></a><a class="docs-heading-anchor-permalink" href="#Plot-the-result" title="Permalink"></a></h3><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.4604320371120217
Epoch 101 ||  0.017117424516105283
Epoch 201 ||  0.016052332416504726
Epoch 301 ||  0.01592918212709066
Epoch 401 ||  0.01575545101831737
Epoch 501 ||  0.015683492612911753
Epoch 601 ||  0.015667397503505088
Epoch 701 ||  0.015614874240651479
Epoch 801 ||  0.015694302296285722
Epoch 901 ||  0.01552732193479201
Epoch 1001 ||  0.015588681014159661
Epoch 1101 ||  0.015389706549236316
Epoch 1201 ||  0.01523513115365125
Epoch 1301 ||  0.014737726851425239
Epoch 1401 ||  0.013711310696307391
Epoch 1501 ||  0.012704984530288392
Epoch 1601 ||  0.01227557815403215
Epoch 1701 ||  0.011775414032272889
Epoch 1801 ||  0.011302914963263735
Epoch 1901 ||  0.010540803321384816
Epoch 2000 ||  0.009750332351591198
 15.303223 seconds (15.05 M allocations: 1.444 GiB, 3.56% gc time, 92.65% compilation time)</code></pre><p><img src="../result1.svg" alt/></p><h2 id="Siren"><a class="docs-heading-anchor" href="#Siren">Siren</a><a id="Siren-1"></a><a class="docs-heading-anchor-permalink" href="#Siren" title="Permalink"></a></h2><p>We use four hidden layers with 50 neurons in each.</p><pre><code class="language-julia hljs">model = Siren(1,50,50,50,50,1; omega = 30f0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, sin),      # 100 parameters
    layer_2 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 88 bytes.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  1.078114016621695
Epoch 101 ||  0.0012574971619962688
Epoch 201 ||  6.704498934705276e-5
Epoch 301 ||  2.747560512859569e-6
Epoch 401 ||  5.903401441690195e-8
Epoch 501 ||  6.014729906742897e-10
Epoch 601 ||  4.334237718197419e-12
Epoch 701 ||  2.87508452045745e-13
Epoch 801 ||  3.894763594886068e-14
Epoch 901 ||  4.015093446730794e-14
Epoch 1001 ||  3.4265241515706477e-14
Epoch 1101 ||  1.743300548692937e-14
Epoch 1201 ||  3.3193590116717246e-14
Epoch 1301 ||  2.82233389037447e-14
Epoch 1401 ||  1.8458646699191495e-14
Epoch 1501 ||  2.0275219933351754e-14
Epoch 1601 ||  1.9707850438231703e-14
Epoch 1701 ||  6.832350009670643e-14
Epoch 1801 ||  3.416831911072254e-14
Epoch 1901 ||  4.9758615101470545e-14
Epoch 2000 ||  4.2386371212254493e-14
  5.872380 seconds (5.67 M allocations: 1.085 GiB, 4.65% gc time, 74.24% compilation time)</code></pre><p><img src="../result.svg" alt/></p><p>As we can see the model overfits the data, and the high frequencies cannot be optimized away. We need to tunning the hyperparameter <code>omega</code></p><pre><code class="language-julia hljs">model = Siren(1,50,50,50,50,1; omega = 10f0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, sin),      # 100 parameters
    layer_2 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 88 bytes.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.7004095742526834
Epoch 101 ||  0.00708360152267202
Epoch 201 ||  0.005234975399915025
Epoch 301 ||  0.0037610343492128218
Epoch 401 ||  0.0026985471466971916
Epoch 501 ||  0.002068882755326797
Epoch 601 ||  0.0014199427146202501
Epoch 701 ||  0.0007819149565498416
Epoch 801 ||  0.0004034987526366432
Epoch 901 ||  0.00023318880695533622
Epoch 1001 ||  0.0001403780891016624
Epoch 1101 ||  8.482467556461048e-5
Epoch 1201 ||  5.603713948720968e-5
Epoch 1301 ||  4.314854512701673e-5
Epoch 1401 ||  3.7149033543332635e-5
Epoch 1501 ||  3.352794633274111e-5
Epoch 1601 ||  3.068973095822343e-5
Epoch 1701 ||  2.8171127982408876e-5
Epoch 1801 ||  2.5841379644916787e-5
Epoch 1901 ||  2.3660258930373393e-5
Epoch 2000 ||  2.162736191960779e-5
  1.268371 seconds (855.10 k allocations: 786.339 MiB, 6.63% gc time)</code></pre><p><img src="../result10.svg" alt/></p><h2 id="Gaussian-activation-function"><a class="docs-heading-anchor" href="#Gaussian-activation-function">Gaussian activation function</a><a id="Gaussian-activation-function-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-activation-function" title="Permalink"></a></h2><p>We can also try using a fully connected net with the <a href="../../#Sophon.gaussian"><code>gaussian</code></a> activation function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), gaussian)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, gaussian),  # 100 parameters
    layer_2 = Dense(50 =&gt; 50, gaussian),  # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, gaussian),  # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, gaussian),  # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 80 bytes.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.2795162025443443
Epoch 101 ||  0.0049937366259712795
Epoch 201 ||  0.0028404371266719445
Epoch 301 ||  0.00034805965787867315
Epoch 401 ||  1.0072950881327576e-5
Epoch 501 ||  2.0999183875418524e-6
Epoch 601 ||  9.122598469475418e-7
Epoch 701 ||  3.668161896135012e-7
Epoch 801 ||  1.6121195588355853e-6
Epoch 901 ||  6.584277550558324e-8
Epoch 1001 ||  2.264773377231827e-8
Epoch 1101 ||  7.277711278240012e-8
Epoch 1201 ||  1.5268114127281503e-9
Epoch 1301 ||  2.6282687877103962e-6
Epoch 1401 ||  1.8023544723596144e-7
Epoch 1501 ||  4.4072470762526394e-6
Epoch 1601 ||  6.4451813333479074e-6
Epoch 1701 ||  1.2588334525733698e-5
Epoch 1801 ||  5.968687643060749e-5
Epoch 1901 ||  5.680117843512136e-6
Epoch 2000 ||  1.089129469973458e-5
  6.178740 seconds (6.48 M allocations: 1.127 GiB, 4.14% gc time, 77.00% compilation time)</code></pre><p><img src="../result2.svg" alt/></p><h2 id="Quadratic-activation-function"><a class="docs-heading-anchor" href="#Quadratic-activation-function">Quadratic activation function</a><a id="Quadratic-activation-function-1"></a><a class="docs-heading-anchor-permalink" href="#Quadratic-activation-function" title="Permalink"></a></h2><p><a href="tutorials/@ref"><code>quadratic</code></a> is much cheaper to compute compared to the Gaussain activation function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), quadratic)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, quadratic),  # 100 parameters
    layer_2 = Dense(50 =&gt; 50, quadratic),  # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, quadratic),  # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, quadratic),  # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 80 bytes.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.2775137723188573
Epoch 101 ||  0.0049616471662621816
Epoch 201 ||  0.004499039499958962
Epoch 301 ||  0.0031971790029437543
Epoch 401 ||  0.0015865479258108167
Epoch 501 ||  0.0005118251901391893
Epoch 601 ||  5.208272162717846e-5
Epoch 701 ||  2.2029125868702538e-6
Epoch 801 ||  2.451245246797909e-6
Epoch 901 ||  2.4989726966004717e-7
Epoch 1001 ||  6.659615742992333e-8
Epoch 1101 ||  0.0003983136822130692
Epoch 1201 ||  3.4334856907636874e-8
Epoch 1301 ||  3.741260371432781e-9
Epoch 1401 ||  5.968971993697966e-5
Epoch 1501 ||  1.938236980046958e-6
Epoch 1601 ||  4.1632149865559e-5
Epoch 1701 ||  7.582787212831776e-6
Epoch 1801 ||  1.0643889356456226e-7
Epoch 1901 ||  0.0006938043412280338
Epoch 2000 ||  4.731049155626251e-8
  5.290839 seconds (5.60 M allocations: 1.081 GiB, 4.63% gc time, 78.39% compilation time)</code></pre><p><img src="../result3.svg" alt/></p><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>&quot;Neural networks suppresse high frequency components&quot; is a misinterpretation of the spectral bias. The accurate way of putting it is that the lower frequencies in the error are optimized first in the optimization process. This can be seen in Siren&#39;s example of overfitting data, where you do not have implicit regularization. The high frequency in the network will never go away because it has fitted the data perfectly.</p><p>Mainstream attributes the phenomenon that neural networks &quot;suppress&quot; high frequencies to gradient descent. This is not the whole picture. Initialization also plays an important role. Siren mitigats this problem by initializing larger weights in the first layer, while activation functions such as gassian have large enough gradients and sufficiently large support of the second derivative with proper hyperparameters. Please refer to <a href="../../references/#sitzmann2020implicit">Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein (2020)</a>, <a href="../../references/#ramasinghe2021beyond">Sameera Ramasinghe, Simon Lucey (2021)</a> and <a href="../../references/#ramasinghe2022regularizing">Sameera Ramasinghe, Lachlan MacDonald, Simon Lucey (2022)</a> if you want to dive deeper into this.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../poisson/">1D Multi-scale Poisson&#39;s Equation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Saturday 11 March 2023 03:49">Saturday 11 March 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
