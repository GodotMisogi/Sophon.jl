<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Fitting a nonlinear discontinuous function · Sophon.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://MilkshakeForReal.github.io/Sophon.jl/tutorials/discontinuous/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Sophon.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Fitting a nonlinear discontinuous function</a><ul class="internal"><li><a class="tocitem" href="#Import-pacakges"><span>Import pacakges</span></a></li><li><a class="tocitem" href="#Dataset"><span>Dataset</span></a></li><li><a class="tocitem" href="#Naive-Neural-Nets"><span>Naive Neural Nets</span></a></li><li><a class="tocitem" href="#Siren"><span>Siren</span></a></li><li><a class="tocitem" href="#Gaussian-activation-function"><span>Gaussian activation function</span></a></li><li><a class="tocitem" href="#Quadratic-activation-function"><span>Quadratic activation function</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li><li><a class="tocitem" href="../poisson/">1D Poisson&#39;s Equation</a></li><li><a class="tocitem" href="../convection/">1D Convection Equation</a></li><li><a class="tocitem" href="../helmholtz/">2D Helmholtz Equation</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Fitting a nonlinear discontinuous function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Fitting a nonlinear discontinuous function</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/MilkshakeForReal/Sophon.jl/blob/main/docs/src/tutorials/discontinuous.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Fitting-a-nonlinear-discontinuous-function"><a class="docs-heading-anchor" href="#Fitting-a-nonlinear-discontinuous-function">Fitting a nonlinear discontinuous function</a><a id="Fitting-a-nonlinear-discontinuous-function-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-a-nonlinear-discontinuous-function" title="Permalink"></a></h1><p>This example is taken from <a href="https://royalsocietypublishing.org/doi/epdf/10.1098/rspa.2020.0334">here</a>. However, we do not use adaptive activation functions. Instead, we show that using suitable non-parametric activation functions immediately performs better.</p><p>The following  discontinuous  function  with  discontinuity  at <span>$x=0$</span>  location  is approximated by <a href="../../#Sophon.Siren-Tuple{Int64, Int64}"><code>Siren</code></a>.</p><p class="math-container">\[u(x)= \begin{cases}0.2 \sin (18 x) &amp; \text { if } x \leq 0 \\ 1+0.3 x \cos (54 x) &amp; \text { otherwise }\end{cases}\]</p><p>The domain is <span>$[-1,1]$</span>. The number of training points used is <code>300</code>.</p><h2 id="Import-pacakges"><a class="docs-heading-anchor" href="#Import-pacakges">Import pacakges</a><a id="Import-pacakges-1"></a><a class="docs-heading-anchor-permalink" href="#Import-pacakges" title="Permalink"></a></h2><pre><code class="language-julia hljs">using Lux, Sophon
using NNlib, Optimisers, Plots, Random, Statistics, Zygote</code></pre><h2 id="Dataset"><a class="docs-heading-anchor" href="#Dataset">Dataset</a><a id="Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Dataset" title="Permalink"></a></h2><pre><code class="language-julia hljs">function u(x)
    if x &lt;= 0
        return 0.2 * sin(18 * x)
    else
        return 1 + 0.3 * x * cos(54 * x)
    end
end

function generate_data(n=300)
    x = reshape(collect(range(-1.0f0, 1.0f0, n)), (1, n))
    y = u.(x)
    return (x, y)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">generate_data (generic function with 2 methods)</code></pre><p>Let&#39;s visualize the data.</p><pre><code class="language-julia hljs">x, y = generate_data()
Plots.plot(vec(x), vec(y),label=false)</code></pre><p><img src="../u.svg" alt/></p><h2 id="Naive-Neural-Nets"><a class="docs-heading-anchor" href="#Naive-Neural-Nets">Naive Neural Nets</a><a id="Naive-Neural-Nets-1"></a><a class="docs-heading-anchor-permalink" href="#Naive-Neural-Nets" title="Permalink"></a></h2><p>First we demonstrate show naive fully connected neural nets could be really bad at fitting this function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), relu)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, relu),     # 100 parameters
    layer_2 = Dense(50 =&gt; 50, relu),    # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, relu),    # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, relu),    # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 80 bytes.</code></pre><h3 id="Train-the-model"><a class="docs-heading-anchor" href="#Train-the-model">Train the model</a><a id="Train-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-model" title="Permalink"></a></h3><pre><code class="language-julia hljs">function train(model)
    ps, st = Lux.setup(Random.default_rng(), model)
    opt = Adam()
    st_opt = Optimisers.setup(opt,ps)
    function loss(model, ps, st, x, y)
        y_pred, _ = model(x, ps, st)
        mes = mean(abs2, y_pred .- y)
        return mes
    end

    for i in 1:2000
        gs = gradient(p-&gt;loss(model,p,st,x,y), ps)[1]
        st_opt, ps = Optimisers.update(st_opt, ps, gs)
        if i % 100 == 1 || i == 2000
            println(&quot;Epoch $i ||  &quot;, loss(model,ps,st,x,y))
        end
    end
    return ps, st
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">train (generic function with 1 method)</code></pre><h3 id="Plot-The-Result"><a class="docs-heading-anchor" href="#Plot-The-Result">Plot The Result</a><a id="Plot-The-Result-1"></a><a class="docs-heading-anchor-permalink" href="#Plot-The-Result" title="Permalink"></a></h3><pre><code class="language-julia hljs">@time ps, st = train(model)
y_pred = model(x,ps,st)[1]
Plots.plot(vec(x), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x), vec(y),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.26894250919194623
Epoch 101 ||  0.018026690025916888
Epoch 201 ||  0.01648795310820358
Epoch 301 ||  0.016102761252706033
Epoch 401 ||  0.01587940102028403
Epoch 501 ||  0.015681005851533453
Epoch 601 ||  0.015422734149558446
Epoch 701 ||  0.015016648638695
Epoch 801 ||  0.014011335351114788
Epoch 901 ||  0.01314955666934849
Epoch 1001 ||  0.012850880289491466
Epoch 1101 ||  0.012672300955706973
Epoch 1201 ||  0.012700440475855139
Epoch 1301 ||  0.0126459029148042
Epoch 1401 ||  0.012543523202632184
Epoch 1501 ||  0.012790168429500329
Epoch 1601 ||  0.012381494461167845
Epoch 1701 ||  0.012234485961999862
Epoch 1801 ||  0.012498198551749367
Epoch 1901 ||  0.010990821762837454
Epoch 2000 ||  0.009427472043098005
 43.970046 seconds (30.51 M allocations: 4.551 GiB, 2.68% gc time, 94.16% compilation time)</code></pre><p><img src="../result1.svg" alt/></p><h2 id="Siren"><a class="docs-heading-anchor" href="#Siren">Siren</a><a id="Siren-1"></a><a class="docs-heading-anchor-permalink" href="#Siren" title="Permalink"></a></h2><p>We use four hidden layers with 50 neurons in each.</p><pre><code class="language-julia hljs">model = Siren(1,50,50,50,50,1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, sin),      # 100 parameters
    layer_2 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, sin),     # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 88 bytes.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model)
y_pred = model(x,ps,st)[1]
Plots.plot(vec(x), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x), vec(y),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  1.0601148086327914
Epoch 101 ||  0.0025953867281252544
Epoch 201 ||  0.0008967544551210571
Epoch 301 ||  0.0006042266616142265
Epoch 401 ||  0.0004879611326089587
Epoch 501 ||  0.00041575150389253855
Epoch 601 ||  0.00035810318188392103
Epoch 701 ||  0.00030642153349701954
Epoch 801 ||  0.0002572224477416822
Epoch 901 ||  0.00020932198556178195
Epoch 1001 ||  0.0001634572710834761
Epoch 1101 ||  0.0001218782042289372
Epoch 1201 ||  8.71316907132706e-5
Epoch 1301 ||  6.0640843437679035e-5
Epoch 1401 ||  4.2219685103352145e-5
Epoch 1501 ||  3.037264017116825e-5
Epoch 1601 ||  2.2983507722456237e-5
Epoch 1701 ||  1.8193108338113525e-5
Epoch 1801 ||  1.4823461052898993e-5
Epoch 1901 ||  1.2264544420636846e-5
Epoch 2000 ||  1.0244170899408766e-5
 21.009212 seconds (18.19 M allocations: 4.810 GiB, 4.40% gc time, 75.02% compilation time)</code></pre><p><img src="../result.svg" alt/></p><h2 id="Gaussian-activation-function"><a class="docs-heading-anchor" href="#Gaussian-activation-function">Gaussian activation function</a><a id="Gaussian-activation-function-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-activation-function" title="Permalink"></a></h2><p>We can also try using a fully connected net with the <a href="../../#Sophon.gaussian"><code>gaussian</code></a> activation function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), gaussian)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, gaussian),  # 100 parameters
    layer_2 = Dense(50 =&gt; 50, gaussian),  # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, gaussian),  # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, gaussian),  # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 80 bytes.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model)
y_pred = model(x,ps,st)[1]
Plots.plot(vec(x), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x), vec(y),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  1.2720791097650452
Epoch 101 ||  0.007989862789697495
Epoch 201 ||  0.006772873586812916
Epoch 301 ||  0.005651516730557757
Epoch 401 ||  0.004715133465744649
Epoch 501 ||  0.003915403382086021
Epoch 601 ||  0.003374906693026938
Epoch 701 ||  0.0023858425529400236
Epoch 801 ||  0.001084195143498369
Epoch 901 ||  0.0003700799821025893
Epoch 1001 ||  0.0001247710221667435
Epoch 1101 ||  8.672388529197038e-5
Epoch 1201 ||  4.446852857040023e-5
Epoch 1301 ||  3.613830862262381e-5
Epoch 1401 ||  2.9831643677014822e-5
Epoch 1501 ||  4.1733941933512324e-5
Epoch 1601 ||  2.3614850966976922e-5
Epoch 1701 ||  2.1049717940493352e-5
Epoch 1801 ||  1.9363866953952496e-5
Epoch 1901 ||  1.7893038861681142e-5
Epoch 2000 ||  1.6270359160088445e-5
 18.113936 seconds (14.29 M allocations: 4.485 GiB, 4.39% gc time, 69.47% compilation time)</code></pre><p><img src="../result2.svg" alt/></p><h2 id="Quadratic-activation-function"><a class="docs-heading-anchor" href="#Quadratic-activation-function">Quadratic activation function</a><a id="Quadratic-activation-function-1"></a><a class="docs-heading-anchor-permalink" href="#Quadratic-activation-function" title="Permalink"></a></h2><p><a href="tutorials/@ref"><code>quadratic</code></a> is much cheaper to compute compared to the Gaussain activation function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), quadratic)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, quadratic),  # 100 parameters
    layer_2 = Dense(50 =&gt; 50, quadratic),  # 2_550 parameters
    layer_3 = Dense(50 =&gt; 50, quadratic),  # 2_550 parameters
    layer_4 = Dense(50 =&gt; 50, quadratic),  # 2_550 parameters
    layer_5 = Dense(50 =&gt; 1),           # 51 parameters
)         # Total: 7_801 parameters,
          #        plus 0 states, summarysize 80 bytes.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model)
y_pred = model(x,ps,st)[1]
Plots.plot(vec(x), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x), vec(y),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.3156394408283924
Epoch 101 ||  0.006978060463572681
Epoch 201 ||  0.005949044796042895
Epoch 301 ||  0.004618548364662591
Epoch 401 ||  0.00278893480723314
Epoch 501 ||  0.0012366849611323983
Epoch 601 ||  0.00046700259323396824
Epoch 701 ||  0.00015568283116136822
Epoch 801 ||  0.0002495036663745267
Epoch 901 ||  5.789848901907983e-5
Epoch 1001 ||  4.416905004324446e-5
Epoch 1101 ||  8.130333389800802e-5
Epoch 1201 ||  3.0654581656175287e-5
Epoch 1301 ||  2.5694905294058117e-5
Epoch 1401 ||  8.163439908028393e-5
Epoch 1501 ||  2.40688920830917e-5
Epoch 1601 ||  1.900828204512497e-5
Epoch 1701 ||  1.6032794513430915e-5
Epoch 1801 ||  1.412230317944664e-5
Epoch 1901 ||  1.2653119248752341e-5
Epoch 2000 ||  0.00031195077290719763
 14.988187 seconds (12.51 M allocations: 4.362 GiB, 4.97% gc time, 77.74% compilation time)</code></pre><p><img src="../result3.svg" alt/></p><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>The so-called &quot;Spectral bias&quot; can be a misleading term. It attributes the phenomenon that neural networks suppress high frequencies to gradient descent. This is not the whole picture. Siren solves this problem by initializing larger weights in the first layer, while activation functions such as gassian have large enough gradients themselves. Please refer to <a href="../../references/#sitzmann2020implicit">Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein (2020)</a>, <a href="../../references/#ramasinghe2021beyond">Sameera Ramasinghe, Simon Lucey (2021)</a> and <a href="../../references/#ramasinghe2022regularizing">Sameera Ramasinghe, Lachlan MacDonald, Simon Lucey (2022)</a> if you want to dive deeper into this.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../poisson/">1D Poisson&#39;s Equation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Saturday 27 August 2022 22:04">Saturday 27 August 2022</span>. Using Julia version 1.8.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
