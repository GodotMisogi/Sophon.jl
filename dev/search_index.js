var documenterSearchIndex = {"docs":
[{"location":"tutorials/helmholtz/#Helmholtz-equation","page":"2D Helmholtz Equation","title":"Helmholtz equation","text":"","category":"section"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"Let us consider the Helmholtz equation in two space dimensions","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"beginaligned\nDelta u(x y)+k^2 u(x y)=q(x y) quad(x y) in Omega=(-11)^2 \nu(x y)=0 quad(x y) in partial Omega\nendaligned","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"where ","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"q(x y)=-left(a_1 piright)^2 sin left(a_1 pi xright) sin left(a_2 pi yright)-left(a_2 piright)^2 sin left(a_1 pi xright) sin left(a_2 pi yright)+k^2 sin left(a_1 pi xright) sin left(a_2 pi yright)","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"The excat solution is u(xy)=sina_1pi xsina_2pi y. We chose k=1 a_1 = 1 and a_2 = 4.","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"using ModelingToolkit, IntervalSets, Sophon, Lux, CUDA\nusing Optimization, OptimizationOptimJL\n\n@parameters x,y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\na1 = 1\na2 = 4\nk = 1\n\nq(x,y) = -(a1*π)^2 * sin(a1*π*x) * sin(a2*π*y) - (a2*π)^2 * sin(a1*π*x) * sin(a2*π*y) + k^2 * sin(a1*π*x) * sin(a2*π*y)\neq = Dxx(u(x,y)) + Dyy(u(x,y)) + k^2 * u(x,y) ~ q(x,y)\ndomains = [x ∈ Interval(-1,1), y ∈ Interval(-1,1)]\nbcs = [u(-1,y) ~ 0, u(1,y) ~ 0, u(x, -1) ~ 0, u(x, 1) ~ 0]\n\n@named helmholtz = PDESystem(eq, bcs, domains, [x,y], [u(x,y)])","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"Note that the boundary conditions are compatible with periocity, which allows us to apply BACON.","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"chain = BACON(2, 1, 5, 2; hidden_dims = 32, num_layers=5)\npinn = PINN(chain) # call `gpu` on it if you want to use gpu\nsampler = QuasiRandomSampler(300, 100)  \nstrategy = NonAdaptiveTraining()\n\nprob = Sophon.discretize(helmholtz, pinn, sampler, strategy) \n\n@time res = Optimization.solve(prob, BFGS(); maxiters=1000)","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"Let's plot the result.","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"phi = pinn.phi\n\nxs, ys= [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nu_analytic(x,y) = sinpi(a1*x)*sinpi(a2*y)\nu_real = [u_analytic(x,y) for x in xs, y in ys]\n\nphi_cpu = cpu(phi) # in case you are using GPU\nps_cpu = cpu(res.u)\nu_pred = [sum(phi_cpu(([x,y]), ps_cpu)) for x in xs, y in ys]\n\nusing CairoMakie\naxis = (xlabel=\"x\", ylabel=\"y\", title=\"Analytical Solution\")\nfig, ax1, hm1 = heatmap(xs, ys, u_real, axis=axis)\nColorbar(fig[:, end+1], hm1)\nax2, hm2= heatmap(fig[1, end+1], xs, ys, u_pred, axis= merge(axis, (;title = \"Prediction\")))\nColorbar(fig[:, end+1], hm2)\nax3, hm3 = heatmap(fig[1, end+1], xs, ys, abs.(u_pred-u_real), axis= merge(axis, (;title = \"Absolute Error\")))\nColorbar(fig[:, end+1], hm3)\nfig\nsave(\"helmholtz.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"(Image: )","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"tutorials/discontinuous/#Fitting-a-nonlinear-discontinuous-function","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"This example is taken from here. However, we do not use adaptive activation functions. Instead, we show that using suitable non-parametric activation functions immediately performs better.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"Consider the following  discontinuous  function  with  discontinuity  at x=0:","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"u(x)= begincases02 sin (18 x)  text  if  x leq 0  1+03 x cos (54 x)  text  otherwise endcases","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"The domain is -11. The number of training points used is 50.","category":"page"},{"location":"tutorials/discontinuous/#Import-pacakges","page":"Fitting a nonlinear discontinuous function","title":"Import pacakges","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"using Lux, Sophon\nusing NNlib, Optimisers, Plots, Random, StatsBase, Zygote","category":"page"},{"location":"tutorials/discontinuous/#Dataset","page":"Fitting a nonlinear discontinuous function","title":"Dataset","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"function u(x)\n    if x <= 0\n        return 0.2 * sin(18 * x)\n    else\n        return 1 + 0.3 * x * cos(54 * x)\n    end\nend\n\nfunction generate_data(n=50)\n    x = reshape(collect(range(-1.0f0, 1.0f0, n)), (1, n))\n    y = u.(x)\n    return (x, y)\nend","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"Let's visualize the data.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"x_train, y_train = generate_data(50)\nx_test, y_test = generate_data(200)\nPlots.plot(vec(x_test), vec(y_test),label=false)\nsavefig(\"u.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Naive-Neural-Nets","page":"Fitting a nonlinear discontinuous function","title":"Naive Neural Nets","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"First we demonstrate show naive fully connected neural nets could be really bad at fitting this function.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = FullyConnected((1,50,50,50,50,1), relu)","category":"page"},{"location":"tutorials/discontinuous/#Train-the-model","page":"Fitting a nonlinear discontinuous function","title":"Train the model","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"function train(model, x, y)\n    ps, st = Lux.setup(Random.default_rng(), model)\n    opt = Adam()\n    st_opt = Optimisers.setup(opt,ps)\n    function loss(model, ps, st, x, y)\n        y_pred, _ = model(x, ps, st)\n        mes = mean(abs2, y_pred .- y)\n        return mes\n    end\n\n    for i in 1:2000\n        gs = gradient(p->loss(model,p,st,x,y), ps)[1]\n        st_opt, ps = Optimisers.update(st_opt, ps, gs)\n        if i % 100 == 1 || i == 2000\n            println(\"Epoch $i ||  \", loss(model,ps,st,x,y))\n        end\n    end\n    return ps, st\nend\n","category":"page"},{"location":"tutorials/discontinuous/#Plot-the-result","page":"Fitting a nonlinear discontinuous function","title":"Plot the result","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result1.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Siren","page":"Fitting a nonlinear discontinuous function","title":"Siren","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"We use four hidden layers with 50 neurons in each.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = Siren(1,50,50,50,50,1; omega = 30f0)","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"As we can see the model overfits the data, and the high frequencies cannot be optimized away. We need to tunning the hyperparameter omega","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = Siren(1,50,50,50,50,1; omega = 10f0)","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result10.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Gaussian-activation-function","page":"Fitting a nonlinear discontinuous function","title":"Gaussian activation function","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"We can also try using a fully connected net with the gaussian activation function.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = FullyConnected((1,50,50,50,50,1), gaussian)","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result2.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Quadratic-activation-function","page":"Fitting a nonlinear discontinuous function","title":"Quadratic activation function","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"quadratic is much cheaper to compute compared to the Gaussain activation function.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = FullyConnected((1,50,50,50,50,1), quadratic)","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result3.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Conclusion","page":"Fitting a nonlinear discontinuous function","title":"Conclusion","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"\"Neural networks suppresse high frequency components\" is a misinterpretation of the spectral bias. The accurate way of putting it is that the lower frequencies in the error are optimized first in the optimization process. This can be seen in Siren's example of overfitting data, where you do not have implicit regularization. The high frequency in the network will never go away because it has fitted the data perfectly.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"Mainstream attributes the phenomenon that neural networks \"suppress\" high frequencies to gradient descent. This is not the whole picture. Initialization also plays an important role. Siren mitigats this problem by initializing larger weights in the first layer, while activation functions such as gassian have large enough gradients and sufficiently large support of the second derivative with proper hyperparameters. Please refer to Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein (2020), Sameera Ramasinghe, Simon Lucey (2021) and Sameera Ramasinghe, Lachlan MacDonald, Simon Lucey (2022) if you want to dive deeper into this.","category":"page"},{"location":"tutorials/allen_cahn/#Allen-Cahn-Equation-with-Sequential-Training","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"","category":"section"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"In this tutorial we are going to solve the Allen-Cahn equation with periodic boundary condition from t=0 to t=1. The traning process is split into four stages, namely  tin 0025, tin 0005, tin 00075 and tin 00 10.","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"using ModelingToolkit, IntervalSets\nusing Sophon\nusing Optimization, OptimizationOptimJL\n\n@parameters t, x\n@variables u(..)\nDₓ = Differential(x)\nDₓ² = Differential(x)^2\nDₜ = Differential(t)\n\neq = Dₜ(u(x, t)) - 0.0001 * Dₓ²(u(x, t)) + 5 * u(x,t) * (abs2(u(x,t)) - 1.0) ~ 0.0\n\ndomain = [x ∈ -1.0..1.0, t ∈ 0.0..0.25]\n\nbcs = [u(x,0) ~ x^2 * cospi(x),\n       u(-1,t) ~ u(1,t)]\n\n@named allen = PDESystem(eq, bcs, domain, [x, t], [u(x, t)])","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"Then we define the neural net, the sampler, and the training strategy.","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"chain = FullyConnected(2, 1, tanh; hidden_dims=16, num_layers=4)\npinn = PINN(chain)\nsampler = QuasiRandomSampler(500, (300, 100))\nstrategy = NonAdaptiveTraining(1, (50, 1))\nprob = Sophon.discretize(allen, pinn, sampler, strategy)","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"We solve the equation sequentially in time.","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"function train(allen, prob, sampler, strategy)\n    bfgs = BFGS()\n    res = Optimization.solve(prob, bfgs; maxiters=2000)\n\n    for tmax in [0.5, 0.75, 1.0]\n        allen.domain[2] = t ∈ 0.0..tmax\n        data = Sophon.sample(allen, sampler, strategy)\n        prob = remake(prob; u0=res.u, p=data)\n        res = Optimization.solve(prob, bfgs; maxiters=2000)\n    end\n    return res\nend\n\nres = train(allen, prob, sampler, strategy)","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"Let's plot the result.","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"using CairoMakie\n\nphi = pinn.phi\nxs, ts = [infimum(d.domain):0.01:supremum(d.domain) for d in allen.domain]\naxis = (xlabel=\"t\", ylabel=\"x\", title=\"Prediction\")\nu_pred = [sum(pinn.phi([x, t], res.u)) for x in xs, t in ts]\nfig, ax, hm = heatmap(ts, xs, u_pred', axis=axis)\n\nsave(\"allen.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"(Image: )","category":"page"},{"location":"tutorials/SchrödingerEquation/#Schrödinger-equation","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger equation","text":"","category":"section"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"The nonlinear Shrödinger equation is given by","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"mathrmi partial_t psi=-frac12 sigma partial_x x psi-betapsi^2 psi","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"Let sigma=beta=1 psi=u+v i, the equation can be transformed into a system of partial differential equations","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"using ModelingToolkit, IntervalSets, Sophon, CairoMakie\nusing Optimization, OptimizationOptimJL\n\n@parameters x,t\n@variables u(..), v(..)\nDₜ = Differential(t)\nDₓ² = Differential(x)^2\n\neqs=[Dₜ(u(x,t)) ~ -Dₓ²(v(x,t))/2 - (abs2(v(x,t)) + abs2(u(x,t))) * v(x,t),\n     Dₜ(v(x,t)) ~  Dₓ²(u(x,t))/2 + (abs2(v(x,t)) + abs2(u(x,t))) * u(x,t)]\n\nbcs = [u(x, 0.0) ~ 2sech(x),\n       v(x, 0.0) ~ 0.0,\n       u(-5.0, t) ~ u(5.0, t),\n       v(-5.0, t) ~ v(5.0, t)]\n\ndomains = [x ∈ Interval(-5.0, 5.0),\n           t ∈ Interval(0.0, π/2)]\n\n@named pde_system = PDESystem(eqs, bcs, domains, [x,t], [u(x,t),v(x,t)])","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"pinn = PINN(u = Siren(2,1; hidden_dims=16,num_layers=4, omega = 1.0),\n            v = Siren(2,1; hidden_dims=16,num_layers=4, omega = 1.0))\n            \nsampler = QuasiRandomSampler(500, (500,500,20,20))\nstrategy = NonAdaptiveTraining(1,(10,10,1,1))\n\nprob = Sophon.discretize(pde_system, pinn, sampler, strategy)","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"Now we train the neural nets and resample data while training.","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"function train(pde_system, prob, sampler, strategy, resample_period = 500, n=10)\n     bfgs = BFGS()\n     res = Optimization.solve(prob, bfgs; maxiters=2000)\n     \n     for i in 1:n\n         data = Sophon.sample(pde_system, sampler, strategy)\n         prob = remake(prob; u0=res.u, p=data)\n         res = Optimization.solve(prob, bfgs; maxiters=resample_period)\n     end\n     return res\nend\n\nres = train(pde_system, prob, sampler, strategy)","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"phi = pinn.phi\nps = res.u\n\nxs, ts= [infimum(d.domain):0.01:supremum(d.domain) for d in pde_system.domain]\n\nu = [sum(phi.u(([x,t]), ps.u)) for x in xs, t in ts]\nv = [sum(phi.v(([x,t]), ps.v)) for x in xs, t in ts]\nψ = @. sqrt(u^2+ v^2)\n\naxis = (xlabel=\"t\", ylabel=\"x\", title=\"u\")\nfig, ax1, hm1 = CairoMakie.heatmap(ts, xs, u', axis=axis)\nax2, hm2= CairoMakie.heatmap(fig[1, end+1], ts, xs, v', axis= merge(axis, (; title=\"v\")))\ndisplay(fig)\nsave(\"uv.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"(Image: )","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"axis = (xlabel=\"t\", ylabel=\"x\", title=\"ψ\")\nfig, ax1, hm1 = CairoMakie.heatmap(ts, xs, ψ', axis=axis, colormap=:jet)\nColorbar(fig[:, end+1], hm1)\ndisplay(fig)\nsave(\"phi.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"(Image: )","category":"page"},{"location":"tutorials/SchrödingerEquation/#Customize-Sampling","page":"Schrödinger Equation: A PDE System with Resampling","title":"Customize Sampling","text":"","category":"section"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"Bascially any sampling method is supportted.","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"using StatsBase\n\ndata = vec([[x, t] for x in xs, t in ts])\nwv = vec(ψ)\nnew_data = wsample(data, wv, 500)\nnew_data = reduce(hcat, new_data)\nfig, ax = scatter(new_data[2,:], new_data[1,:])\nsave(\"data.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"(Image: )","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"prob.p[1] = new_data\nprob.p[2] = new_data\nprob = remake(prob; u0 = res.u)\n# res = Optimization.solve(prob, bfgs; maxiters=1000)","category":"page"},{"location":"tutorials/convection/#D-Convection-Equation","page":"1D Convection Equation","title":"1D Convection Equation","text":"","category":"section"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"Consider the following 1D-convection equation with periodic boundary conditions.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"beginaligned\nfracpartial upartial t+c fracpartial upartial x=0 x in01 t in01 \nu(x 0)=sin(2pi x) \nendaligned","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"First we define the PDE.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"using ModelingToolkit, Sophon, IntervalSets, CairoMakie\nusing Optimization, OptimizationOptimJL\n\n@parameters x, t\n@variables u(..)\nDₜ = Differential(t)\nDₓ = Differential(x)\n\nc = 6\neq = Dₜ(u(x,t)) + c * Dₓ(u(x,t)) ~ 0\nu_analytic(x,t) = sinpi(2*(x-c*t))\n\ndomains = [x ∈ 0..1, t ∈ 0..1]\n\nbcs = [u(x,0) ~ u_analytic(x,0)]\n\n@named convection = PDESystem(eq, bcs, domains, [x,t], [u(x,t)])","category":"page"},{"location":"tutorials/convection/#Imposing-periodic-boundary-conditions","page":"1D Convection Equation","title":"Imposing periodic boundary conditions","text":"","category":"section"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"We will use BACON to impose the boundary conditions. To this end, we simply set period to be one.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"chain = BACON(2, 1, 8, 1; hidden_dims = 32, num_layers=4)","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"note: Note\nFor demonstration purposes, the model is also periodic in time","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"sampler = QuasiRandomSampler(500, 100) # data points\nstrategy = NonAdaptiveTraining(1 , 500) # weights\npinn = PINN(chain)\n\nprob = Sophon.discretize(convection, pinn, sampler, strategy) \n\n@time res = Optimization.solve(prob, BFGS(); maxiters = 1000)","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"Let's visualize the result.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"phi = pinn.phi\n\nxs, ts= [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nu_pred = [sum(phi([x,t],res.u)) for x in xs, t in ts]\nu_real = u_analytic.(xs,ts')\nfig, ax, hm = CairoMakie.heatmap(ts, xs, u_pred', axis=(xlabel=\"t\", ylabel=\"x\", title=\"c = $c\"))\nax2, hm2 = heatmap(fig[1,end+1], ts,xs, abs.(u_pred' .- u_real'), axis = (xlabel=\"t\", ylabel=\"x\", title=\"Absolute error\"))\nColorbar(fig[:, end+1], hm2)\ndisplay(fig)\nsave(\"convection.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"(Image: )","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"We can verify that our model is indeed, periodic.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"xs, ts= [infimum(d.domain):0.01:supremum(d.domain)*2 for d in domains]\nu_pred = [sum(phi([x,t],res.u)) for x in xs, t in ts]\nfig, ax, hm = CairoMakie.heatmap(ts, xs, u_pred', axis=(xlabel=\"t\", ylabel=\"x\", title=\"c = $c\"))\ndisplay(fig)\nsave(\"convection2.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"(Image: )","category":"page"},{"location":"tutorials/poisson/#D-Poisson's-Equation","page":"1D Multi-scale Poisson's Equation","title":"1D Poisson's Equation","text":"","category":"section"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"This example is taken from Sifan Wang, Hanwen Wang, Paris Perdikaris (2021). Consider a simple 1D Poisson’s equation with Dirichlet boundary conditions. The solution is given by","category":"page"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"u(x)=sin (2 pi x)+01 sin (50 pi x)","category":"page"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"using ModelingToolkit, IntervalSets, Sophon\nusing Optimization, OptimizationOptimJL\nusing CairoMakie\n\n@parameters x\n@variables u(..)\nDₓ² = Differential(x)^2\n\nf(x) = -4 * π^2 * sin(2 * π * x) - 250 * π^2 * sin(50 * π * x)\neq = Dₓ²(u(x)) ~ f(x)\ndomain = [x ∈ 0 .. 1]\nbcs = [u(0) ~ 0, u(1) ~ 0]\n\n@named poisson = PDESystem(eq, bcs, domain, [x], [u(x)])","category":"page"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"chain = Siren(1, 16, 32, 16, 1)\npinn = PINN(chain)\nsampler = QuasiRandomSampler(200, 1) \nstrategy = NonAdaptiveTraining(1 , 50)\n\nprob = Sophon.discretize(poisson, pinn, sampler, strategy)\nres = Optimization.solve(prob, BFGS(); maxiters=2000)\n\nphi = pinn.phi\nxs = 0:0.001:1\nu_true = @. sin(2 * pi * xs) + 0.1 * sin(50 * pi * xs)\nus = phi(xs', res.u)\nfig = Figure()\naxis = Axis(fig[1, 1])\nlines!(xs, u_true; label=\"Ground Truth\")\nlines!(xs, vec(us); label=\"Prediction\")\naxislegend(axis)\nfig\nsave(\"result.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"(Image: )","category":"page"},{"location":"tutorials/poisson/#Compute-the-relative-L2-error","page":"1D Multi-scale Poisson's Equation","title":"Compute the relative L2 error","text":"","category":"section"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"using Integrals\n\nu_analytical(x,p) = sin.(2 * pi .* x) + 0.1 * sin.(50 * pi .* x)\nerror(x,p) = abs2.(vec(phi([x;;],res.u)) .- u_analytical(x,p))\n\nrelative_L2_error = solve(IntegralProblem(error,0,1),HCubatureJL(),reltol=1e-3,abstol=1e-3) ./ solve(IntegralProblem((x,p) -> abs2.(u_analytical(x,p)),0, 1),HCubatureJL(),reltol=1e-3,abstol=1e-3)","category":"page"},{"location":"tutorials/inverse/#Inverse-Problem-of-the-Lorenz-System","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"","category":"section"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"fracmathrmd xmathrmd t=sigma(y-x)","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":",","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"fracmathrmd ymathrmd t=x(rho-z)-y","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":",","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"fracmathrmd zmathrmd t=x y-beta z","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":",","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"using ModelingToolkit, Sophon, OrdinaryDiffEq\nusing Optimization, OptimizationOptimJL\nusing ModelingToolkit, IntervalSets","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"@parameters t \n@variables x(..), y(..), z(..), σ(..), β(..), ρ(..)\n\nDt = Differential(t)\neqs = [Dt(x(t)) ~ σ(t)*(y(t) - x(t)),\n       Dt(y(t)) ~ x(t)*(ρ(t) - z(t)) - y(t),\n       Dt(z(t)) ~ x(t)*y(t) - β(t)*z(t)]\n\nbcs = [x(0) ~ 1.0, y(0) ~ 0.0, z(0) ~ 0.0]\ndomains = [t ∈ Interval(0.0,1.0)]\n@named pde_system = PDESystem(eqs, bcs, domains, [t], [x(t),y(t),z(t),σ(t), ρ(t), β(t)])","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"function lorenz!(du,u,p,t)\n    du[1] = 10.0*(u[2]-u[1])\n    du[2] = u[1]*(28.0-u[3]) - u[2]\n    du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,1.0)\nprob = ODEProblem(lorenz!,u0,tspan)\nsol = solve(prob, Tsit5(), dt=0.1)\nts = [infimum(d.domain):0.1:supremum(d.domain) for d in domains][1]\nfunction getData(sol)\n    data = []\n    us = hcat(sol(ts).u...)\n    ts_ = hcat(sol(ts).t...)\n    return [us,ts_]\nend\ndata = getData(sol)\n\n(u_ , t_) = data","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"pinn = PINN(x = FullyConnected((1,16,16,16,1), tanh),\n            y = FullyConnected((1,16,16,16,1), tanh),\n            z = FullyConnected((1,16,16,16,1), tanh),\n            σ = ConstantFunction(),\n            ρ = ConstantFunction(),\n            β = ConstantFunction())\nsampler = QuasiRandomSampler(100, 1)\nstrategy = NonAdaptiveTraining()\n\nt_data = t_\nu_data = u_ \nfunction additional_loss(phi, θ)\n    return sum(abs2, vcat(phi.x(t_data, θ.x), phi.y(t_data, θ.y), phi.z(t_data, θ.z)).-u_data)/length(t_data)\nend\nprob = Sophon.discretize(pde_system, pinn, sampler, strategy, additional_loss=additional_loss)\n\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n@time res = Optimization.solve(prob, BFGS(), callback = callback, maxiters=1000)","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"print(res.u.σ.constant, res.u.ρ.constant, res.u.β.constant)","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"phi=pinn.phi\nθ = res.u\nts=  [0:0.01:1...;;]\nx_pred = phi.x(ts, θ.x)\ny_pred = phi.x(ts, θ.y)\nz_pred = phi.x(ts, θ.z)","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"using Plots\nPlots.plot(vec(ts), [vec(x_pred),vec(y_pred),vec(z_pred)],  label=[\"x(t)\" \"y(t)\" \"z(t)\"])   ","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Sophon","category":"page"},{"location":"#Sophon","page":"Home","title":"Sophon","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Sophon.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [Sophon]\nPrivate = false","category":"page"},{"location":"#Sophon.AdaptiveTraining","page":"Home","title":"Sophon.AdaptiveTraining","text":"AdaptiveTraining(pde_weights, bcs_weights)\n\nAdaptive weights for the loss functions. Here pde_weights and bcs_weights are functions that take in (phi, x, θ) and return the point-wise weights. Note that bcs_weights can be real numbers but they will be converted to functions that return the same numbers.\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.ChainState","page":"Home","title":"Sophon.ChainState","text":"ChainState(model, rng::AbstractRNG=Random.default_rng())\n\nIt this similar to Lux.Chain but wraps it in a stateful container.\n\nFields\n\nmodel: The neural network.\nstates: The states of the neural network.\n\nInput\n\nx: The input to the neural network.\nps: The parameters of the neural network.\n\nArguments\n\nmodel: AbstractExplicitLayer, or a named tuple of them, which will be treated as a Chain.\nrng: AbstractRNG to use for initialising the neural network.\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.ConstantFunction","page":"Home","title":"Sophon.ConstantFunction","text":"ConstantFunction()\n\nA conatiner for scalar parameter. This is useful for the case that you want a dummy layer that returns the scalar parameter for any input.\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.DeepONet","page":"Home","title":"Sophon.DeepONet","text":"DeepONet(branch_net, trunk_net;\n         flatten_layer=FlattenLayer(),\n         linear_layer=NoOpLayer(),\n         bias=ScalarLayer())\nDeepONet(layer_sizes_branch, activation_branch,\n         layer_sizes_trunk,\n         activation_trunk,\n         layer_sizes_linear=nothing)\n\nDeep operator network. Note that the branch net supports multi-dimensional inputs. The flatten_layer flatten the output of the branch net to a matrix, and the linear_layer is applied to the flattened. In this case, linear_layer must be given to transform the flattened matrix to the correct shape.\n\nv → branch_net → flatten_layer → linear_layer → b\n                                                  ↘\n                                                    b' * t + bias → u\n                                                  ↗\n                                ξ → trunk_net → t\n\nArguments\n\nbranch_net: The branch net.\ntrunk_net: The trunk net.\n\nKeyword Arguments\n\nflatten_layer: The layer to flatten a multi-dimensional array to a matrix.\nlinear_layer: The layer to apply a linear transformation to the output of the flatten_layer.\n\nInputs\n\n(v, ξ): v is an array of shape (b_1b_2b_d m), where d is the dimension of the input function, and m is the number of input functions. ξ is a matrix of shape (d n), where d is the dimension of the output function, and m is the number of \"sensors\".\n\nReturns\n\nA matrix of shape (m n).\n\nExamples\n\njulia> deeponet = DeepONet((3, 5, 4), relu, (2, 6, 4, 4), tanh)\nDeepONet(\n    branch_net = Chain(\n        layer_1 = Dense(3 => 5, relu),  # 20 parameters\n        layer_2 = Dense(5 => 4),        # 24 parameters\n    ),\n    trunk_net = Chain(\n        layer_1 = Dense(2 => 6, tanh_fast),  # 18 parameters\n        layer_2 = Dense(6 => 4, tanh_fast),  # 28 parameters\n        layer_3 = Dense(4 => 4, tanh_fast),  # 20 parameters\n    ),\n    flatten_layer = FlattenLayer(),\n    linear_layer = NoOpLayer(),\n    bias = ScalarLayer(),                    # 1 parameters\n)         # Total: 111 parameters,\n          #        plus 0 states, summarysize 80 bytes.\n\nReference\n\nLu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, George Em Karniadakis (2021)\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.DiscreteFourierFeature","page":"Home","title":"Sophon.DiscreteFourierFeature","text":"DiscreteFourierFeature(in_dims::Int, out_dims::Int, N::Int, period::Real)\n\nThe discrete Fourier filter proposed in David B. Lindell, Dave Van Veen, Jeong Joon Park, Gordon Wetzstein (2021). For a periodic function with period P, the Fourier series in amplitude-phase form is\n\ns_N(x)=fraca_02+sum_n=1^Na_ncdot sin left( frac2piPnx+varphi _n right)\n\nThe output is guaranteed to be periodic.\n\nArguments\n\nin_dims: Number of the input dimensions.\nout_dims: Number of the output dimensions.\nN: N in the formula.\nperiod: P in the formula.\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.FactorizedDense","page":"Home","title":"Sophon.FactorizedDense","text":"FactorizedDense(in_dims::Int, out_dims::Int, activation=identity;\n                     mean::AbstractFloat=1.0f0, std::AbstractFloat=0.1f0,\n                     init_weight=kaiming_uniform(activation), init_bias=zeros32)\n\nCreate a Dense layer where the weight is factorized into twa parts, the scaling factors for each row and the weight matrix.\n\nArguments\n\nin_dims: number of input dimensions\nout_dims: number of output dimensions\nactivation: activation function\n\nKeyword Arguments\n\nmean: mean of the scaling factors\nstd: standard deviation of the scaling factors\ninit_weight: weight initialization function\ninit_bias: bias initialization function\n\nInput\n\nx: input vector or matrix\n\nReturns\n\ny = activation.(scale * weight * x+ bias).\nEmpty NamedTuple().\n\nParameters\n\nscale: scaling factors. Shape: (out_dims, 1)\nweight: Weight Matrix of size (out_dims, in_dims).\nbias: Bias of size (out_dims, 1).\n\nReferences\n\nSifan Wang, Hanwen Wang, Jacob H Seidman, Paris Perdikaris (2022)\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.FourierFeature","page":"Home","title":"Sophon.FourierFeature","text":"FourierFeature(in_dims::Int, std::NTuple{N,Pair{S,T}}) where {N,S,T<:Int}\nFourierFeature(in_dims::Int, frequencies::NTuple{N, T}) where {N, T <: Real}\nFourierFeature(in_dims::Int, out_dims::Int, std::Real)\n\nFourier Feature Network.\n\nArguments\n\nin_dims: Number of the input dimensions.\nstd: A tuple of pairs of sigma => out_dims, where sigma is the standard deviation of the Gaussian distribution.\n\nphi^(i)(x)=leftsin left(2 pi W^(i) xright)  cos 2 pi W^(i) xright W^(i) sim mathcalNleft(0 sigma^(i)right) iin 1 dots D\n\nfrequencies: A tuple of frequencies (f1f2fn).\n\nphi^(i)(x)=leftsin left(2 pi f_i xright)  cos 2 pi f_i xright\n\nParameters\n\nIf std is used, then parameters are Ws in the formula.\n\nInputs\n\nx: AbstractArray with size(x, 1) == in_dims.\n\nReturns\n\nphi^(1) phi^(2)  phi^(D) with size(y, 1) == sum(last(modes) * 2).\n\nExamples\n\njulia> f = FourierFeature(2,10,1) # Random Fourier Feature\nFourierFeature(2 => 10)\n\njulia> f = FourierFeature(2, (1 => 3, 50 => 4)) # Multi-scale Random Fourier Features\nFourierFeature(2 => 14)\n\njulia>  f = FourierFeature(2, (1,2,3,4)) # Predefined frequencies\nFourierFeature(2 => 16)\n\nReferences\n\nAli Rahimi, Benjamin Recht (2007)\n\nMatthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, Ren Ng (2020)\n\nSifan Wang, Hanwen Wang, Paris Perdikaris (2021)\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.NonAdaptiveTraining","page":"Home","title":"Sophon.NonAdaptiveTraining","text":"NonAdaptiveTraining(pde_weights=1, bcs_weights=pde_weights)\n\nFixed weights for the loss functions.\n\nArguments\n\npde_weights: weights for the PDE loss functions. If a single number is given, it is used for all PDE loss functions.\nbcs_weights: weights for the boundary conditions loss functions. If a single number is given, it is used for all boundary conditions loss functions.\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.PINN","page":"Home","title":"Sophon.PINN","text":"PINN(chain, rng::AbstractRNG=Random.default_rng())\nPINN(rng::AbstractRNG=Random.default_rng(); kwargs...)\n\nA container for a neural network, its states and its initial parameters. Call gpu and cpu to move the neural network to the GPU and CPU respectively. The default element type of the parameters is Float64.\n\nFields\n\nphi: ChainState if there is only one neural network, or an named tuple of ChainStates if there are multiple neural networks. The names are the same as the dependent variables in the PDE.\ninit_params: The initial parameters of the neural network.\n\nArguments\n\nchain: AbstractExplicitLayer or a named tuple of AbstractExplicitLayers.\nrng: AbstractRNG to use for initialising the neural network. If yout want to set the seed, write\n\nusing Random\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nand pass rng to PINN as\n\nusing Sophon\n\nchain = FullyConnected((1,6,6,1), sin);\n\n# sinple dependent varibale\npinn = PINN(chain, rng);\n\n# multiple dependent varibales\npinn = PINN(rng;\n            a = chain,\n            b = chain);\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.PINNAttention","page":"Home","title":"Sophon.PINNAttention","text":"PINNAttention(H_net, U_net, V_net, fusion_layers)\nPINNAttention(in_dims::Int, out_dims::Int, activation::Function=sin;\n              hidden_dims::Int, num_layers::Int)\n\nThe output dimesion of H_net and the input dimension of fusion_layers must be the same. For the second and the third constructor, Dense layers is used for H_net, U_net, and V_net. Note that the first constructer does not contain the output layer, but the second one does.\n\n                 x → U_net → u                           u\n                               ↘                           ↘\nx → H_net →  h1 → fusionlayer1 → connection → fusionlayer2 → connection\n                               ↗                           ↗\n                 x → V_net → v                           v\n\nArguments\n\nH_net: AbstractExplicitLayer.\nU_net: AbstractExplicitLayer.\nV_net: AbstractExplicitLayer.\nfusion_layers: Chain.\n\nKeyword Arguments\n\nnum_layers: The number of hidden layers.\nhidden_dims: The number of hidden dimensions of each hidden layer.\n\nReference\n\nSifan Wang, Yujun Teng, Paris Perdikaris (2021)\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.QuasiRandomSampler","page":"Home","title":"Sophon.QuasiRandomSampler","text":"QuasiRandomSampler(pde_points, bcs_points=pde_points;\n                   sampling_alg=SobolSample())\n\nSampler to generate the datasets for PDE and boundary conditions using a quisa-random sampling algorithm. You can call sample(pde, sampler, strategy) on it to generate all the datasets. See QuasiMonteCarlo.jl for available sampling algorithms. The default element type of the sampled data is Float64. The initial sampled data lives on GPU if PINN is. You will need manually move the data to GPU if you want to resample.\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.RBF","page":"Home","title":"Sophon.RBF","text":"RBF(in_dims::Int, out_dims::Int, num_centers::Int=out_dims; sigma::AbstractFloat=0.2f0)\n\nNormalized Radial Basis Fuction Network.\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.ScalarLayer","page":"Home","title":"Sophon.ScalarLayer","text":"ScalarLayer(connection::Function)\n\nReturn connection(scalar, x)\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.SplitFunction","page":"Home","title":"Sophon.SplitFunction","text":"SplitFunction(indices...)\n\nSplit the input along the first demision according to indices.\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.TriplewiseFusion","page":"Home","title":"Sophon.TriplewiseFusion","text":"TriplewiseFusion(connection, layers...)\n\n     u1                    u2\n        ↘                     ↘\nh1 → layer1 → connection → layer2 → connection\n        ↗                     ↗\n     v1                    v2\n\nArguments\n\nconnection: A functio takes 3 inputs and combines them.\nlayers: AbstractExplicitLayers or a Chain.\n\nInputs\n\nLayer behaves differently based on input type:\n\nA tripe of (h, u, v), where u and v itself are tuples of length N, the layers is also a tuple of length N. The computation is as follows\n\nfor i in 1:N\n    h = connection(layers[i](h), u[i], v[i])\nend\n\nA triple of (h, u, v), where u and v are AbstractArrays.\n\nfor i in 1:N\n    h = connection(layers[i](h), u, v)\nend\n\nParameters\n\nParameters of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nStates\n\nStates of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\n\n\n\n\n","category":"type"},{"location":"#Sophon.BACON-Tuple{Int64, Int64, Int64, Real}","page":"Home","title":"Sophon.BACON","text":"BACON(in_dims::Int, out_dims::Int, N::Int, period::Real; hidden_dims::Int, num_layers::Int)\n\nBand-limited Coordinate Networks (BACON) from David B. Lindell, Dave Van Veen, Jeong Joon Park, Gordon Wetzstein (2021). Similar to FourierFilterNet but the frequcies are dicrete and nontrainable.\n\nTips: It is recommended to set period to be 1,2,π or 2π for better performance.\n\n\n\n\n\n","category":"method"},{"location":"#Sophon.FourierAttention-Tuple{Int64, Int64, Function, Any}","page":"Home","title":"Sophon.FourierAttention","text":"FourierAttention(in_dims::Int, out_dims::Int, activation::Function, std;\n                 hidden_dims::Int=512, num_layers::Int=6, modes::NTuple)\nFourierAttention(in_dims::Int, out_dims::Int, activation::Function, frequencies;\n                 hidden_dims::Int=512, num_layers::Int=6, modes::NTuple)\n\nA model that combines FourierFeature and PINNAttention.\n\nx → [FourierFeature(x); x] → PINNAttention\n\nArguments\n\nin_dims: The input dimension.\nout_dims: The output dimension.\nactivation: The activation function.\nstd: See FourierFeature.\nfrequencies: See FourierFeature.\n\nKeyword Arguments\n\nhidden_dim: The hidden dimension of each hidden layer.\nnum_layers: The number of hidden layers.\n\nExamples\n\njulia> FourierAttention(3, 1, sin, (1 => 10, 10 => 10, 50 => 10); hidden_dims=10, num_layers=3)\nChain(\n    layer_1 = SkipConnection(\n        FourierFeature(3 => 60),\n        vcat\n    ),\n    layer_2 = PINNAttention(\n        H_net = Dense(63 => 10, sin),   # 640 parameters\n        U_net = Dense(63 => 10, sin),   # 640 parameters\n        V_net = Dense(63 => 10, sin),   # 640 parameters\n        fusion = TriplewiseFusion(\n            layers = (layer_1 = Dense(10 => 10, sin), layer_2 = Dense(10 => 10, sin), layer_3 = Dense(10 => 10, sin), layer_4 = Dense(10 => 10, sin)),  # 440 parameters\n        ),\n    ),\n    layer_3 = Dense(10 => 1),           # 11 parameters\n)         # Total: 2_371 parameters,\n          #        plus 90 states, summarysize 192 bytes\n\n\n\n\n\n","category":"method"},{"location":"#Sophon.FourierFilterNet-Tuple{Int64, Int64}","page":"Home","title":"Sophon.FourierFilterNet","text":"FourierFilterNet(in_dims::Int, out_dims::Int; hidden_dims::Int, num_layers::Int,\n                 bandwidth::Real)\n\nKeyword Arguments\n\nbandwidth: The maximum bandwidth of the network. The bandwidth is the sum of each filter's bandwidth.\n\nParameters\n\nParameters of the filters:\n\n    Wsim mathcalU(-fracωn fracωn) quad bsim mathcalU(-pi pi)\n\nwhere n is the number of filters.\n\nFor a periodic function with period P, the Fourier series in amplitude-phase form is\n\ns_N(x)=fraca_02+sum_n=1^Na_ncdot sin left( frac2piPnx+varphi _n right)\n\nWe have the following relation between the banthwidth and the parameters of the model:\n\nω = 2πB=frac2πNP\n\nwhere B is the bandwidth of the network.\n\nReferences\n\nRizal Fathony, Anit Kumar Sahu, Devin Willmott, J Zico Kolter (2021)\n\nDavid B. Lindell, Dave Van Veen, Jeong Joon Park, Gordon Wetzstein (2021)\n\n\n\n\n\n","category":"method"},{"location":"#Sophon.FourierNet-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function, Tuple{Vararg{T, N}} where {N, T}}} where {N, T<:Int64}","page":"Home","title":"Sophon.FourierNet","text":"FourierNet(ayer_sizes::NTuple, activation, modes::NTuple)\n\nA model that combines FourierFeature and FullyConnected.\n\nx → FourierFeature → FullyConnected → y\n\nArguments\n\nin_dims: The number of input dimensions.\nlayer_sizes: A tuple of hidden dimensions used to construct FullyConnected.\nactivation: The activation function used to construct FullyConnected.\nmodes: A tuple of modes used to construct FourierFeature.\n\nExamples\n\njulia> FourierNet((2, 30, 30, 1), sin, (1 => 10, 10 => 10, 50 => 10))\nChain(\n    layer_1 = FourierFeature(2 => 60),\n    layer_2 = Dense(60 => 30, sin),     # 1_830 parameters\n    layer_3 = Dense(30 => 30, sin),     # 930 parameters\n    layer_4 = Dense(30 => 1),           # 31 parameters\n)         # Total: 2_791 parameters,\n          #        plus 60 states, summarysize 112 bytes.\n\njulia> FourierNet((2, 30, 30, 1), sin, (1, 2, 3, 4))\nChain(\n    layer_1 = FourierFeature(2 => 16),\n    layer_2 = Dense(16 => 30, sin),     # 510 parameters\n    layer_3 = Dense(30 => 30, sin),     # 930 parameters\n    layer_4 = Dense(30 => 1),           # 31 parameters\n)         # Total: 1_471 parameters,\n          #        plus 4 states, summarysize 96 bytes.\n\n\n\n\n\n","category":"method"},{"location":"#Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T<:Int64}","page":"Home","title":"Sophon.FullyConnected","text":"FullyConnected(layer_sizes::NTuple{N, Int}, activation; outermost = true,\n               init_weight = kaiming_uniform(activation),\n               init_bias = zeros32)\nFullyConnected(in_dims::Int, out_dims::Int, activation::Function;\n               hidden_dims::Int, num_layers::Int, outermost=true,\n               init_weight = kaiming_uniform(activation),\n               init_bias = zeros32)\n\nCreate fully connected layers.\n\nArguments\n\nlayer_sizes: Number of dimensions of each layer.\nhidden_dims: Number of hidden dimensions.\nnum_layers: Number of layers.\nactivation: Activation function.\n\nKeyword Arguments\n\noutermost: Whether to use activation function for the last layer. If false, the activation function is applied to the output of the last layer.\ninit_weight: Initialization method for the weights.\n\nExample\n\njulia> fc = FullyConnected((1, 12, 24, 32), relu)\nChain(\n    layer_1 = Dense(1 => 12, relu),     # 24 parameters\n    layer_2 = Dense(12 => 24, relu),    # 312 parameters\n    layer_3 = Dense(24 => 32),          # 800 parameters\n)         # Total: 1_136 parameters,\n          #        plus 0 states, summarysize 48 bytes.\n\njulia> fc = FullyConnected(1, 10, relu; hidden_dims=20, num_layers=3)\nChain(\n    layer_1 = Dense(1 => 20, relu),     # 40 parameters\n    layer_2 = Dense(20 => 20, relu),    # 420 parameters\n    layer_3 = Dense(20 => 20, relu),    # 420 parameters\n    layer_4 = Dense(20 => 10),          # 210 parameters\n)         # Total: 1_090 parameters,\n          #        plus 0 states, summarysize 64 bytes.\n\n\n\n\n\n","category":"method"},{"location":"#Sophon.Sine-Union{Tuple{Pair{T, T}}, Tuple{T}} where T<:Int64","page":"Home","title":"Sophon.Sine","text":"Sine(in_dims::Int, out_dims::Int; omega::Real)\n\nSinusoidal layer.\n\nExample\n\ns = Sine(2, 2; omega=30.0f0) # first layer\ns = Sine(2, 2) # hidden layer\n\n\n\n\n\n","category":"method"},{"location":"#Sophon.Siren-Tuple{Int64, Int64}","page":"Home","title":"Sophon.Siren","text":"Siren(in_dims::Int, out_dims::Int; hidden_dims::Int, num_layers::Int, omega=30.0f0,\n      init_weight=nothing))\nSiren(layer_sizes::Int...; omega=30.0f0, init_weight=nothing)\n\nSinusoidal Representation Network.\n\nKeyword Arguments\n\nomega: The ω₀ used for the first layer.\ninit_weight: The initialization algorithm for the weights of the input layer. Note that all hidden layers use kaiming_uniform as the initialization algorithm. The default is\n    Wsim mathcalUleft(-fracomegafan_in fracomegafan_inright)\n\nExamples\n\njulia> Siren(2, 32, 32, 1; omega=5.0f0)\nChain(\n    layer_1 = Dense(2 => 32, sin),      # 96 parameters\n    layer_2 = Dense(32 => 32, sin),     # 1_056 parameters\n    layer_3 = Dense(32 => 1),           # 33 parameters\n)         # Total: 1_185 parameters,\n          #        plus 0 states, summarysize 48 bytes.\n\njulia> Siren(3, 1; hidden_dims=20, num_layers=3)\nChain(\n    layer_1 = Dense(3 => 20, sin),      # 80 parameters\n    layer_2 = Dense(20 => 20, sin),     # 420 parameters\n    layer_3 = Dense(20 => 20, sin),     # 420 parameters\n    layer_4 = Dense(20 => 1),           # 21 parameters\n)         # Total: 941 parameters,\n          #        plus 0 states, summarysize 64 bytes.\n\n# Use your own initialization algorithm for the input layer.\njulia> init_weight(rng::AbstractRNG, out_dims::Int, in_dims::Int) = randn(rng, Float32, out_dims, in_dims) .* 2.5f0\njulia> chain = Siren(2, 1; num_layers = 4, hidden_dims = 50, init_weight = init_weight)\n\nReference\n\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein (2020)\n\n\n\n\n\n","category":"method"},{"location":"#Sophon.discretize-Tuple{Any, PINN, Sophon.PINNSampler, Sophon.AbstractTrainingAlg}","page":"Home","title":"Sophon.discretize","text":" discretize(pde_system::PDESystem, pinn::PINN, sampler::PINNSampler,\n                strategy::AbstractTrainingAlg;\n                additional_loss)\n\nConvert the PDESystem into an OptimizationProblem. You can have access to each loss function by calling Sophon.residual_function_1, Sophon.residual_function_2... after calling this function.\n\n\n\n\n\n","category":"method"},{"location":"#Sophon.gaussian","page":"Home","title":"Sophon.gaussian","text":"gaussian(x, a=0.2)\n\nThe Gaussian activation function.\n\ne^frac- x^22a^2\n\nReference\n\nSameera Ramasinghe, Simon Lucey (2021)\n\n\n\n\n\n","category":"function"},{"location":"tutorials/L_shape/#Poisson-equation-over-an-L-shaped-domain","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"","category":"section"},{"location":"tutorials/L_shape/","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"The example is taken from here. We showcase define a PDE on an L-shaped domain","category":"page"},{"location":"tutorials/L_shape/","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"using ModelingToolkit, DomainSets, Optimization, OptimizationOptimJL\nusing DomainSets: ×\nusing Sophon\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\neq = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -1.0\neqs = [eq => (-1..0) × (-1..0),\n       eq => (-1..0) × (0..1),\n       eq => (0..1) × (-1..0)]\n\nbc = u(x,y) ~ 0.0\nboundaries = [(-1 .. -1) × (-1..1),\n              (-1..0) × (1..1),\n              (0..0) × (0..1),\n              (0..1) × (0..0),\n              (1..1) × (-1..0),\n              (-1..1) × (-1 .. -1)]\n\nbcs = [bc => boundary for boundary in boundaries]\n\npde_system = Sophon.PDESystem(eqs, bcs, [x,y], [u(x,y)])","category":"page"},{"location":"tutorials/L_shape/","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"chain = FullyConnected((2,16,16,16,1), tanh)\npinn = PINN(chain)\nsampler = QuasiRandomSampler(300, 30)\nstrategy = NonAdaptiveTraining()\n\nprob = Sophon.discretize(pde_system, pinn, sampler, strategy)\n\nres = Optimization.solve(prob, BFGS(); maxiters=1000)\n\nusing CairoMakie\n\nxs = -1:0.01:1\nys = -1:0.01:1\n\nu_pred = [ifelse(x>0.0 && y>0.0, NaN, pinn.phi([x,y], res.u)[1]) for x in xs, y in ys]\nfig, ax, hm = heatmap(xs, ys, u_pred, colormap=:jet)\nColorbar(fig[:, end+1], hm)\nfig\nsave(\"Lshape.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/L_shape/","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"(Image: )","category":"page"}]
}
